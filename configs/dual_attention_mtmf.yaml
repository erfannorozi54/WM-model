# Dual Attention Multi-Task Multi-Feature
experiment_name: wm_dual_attention_mtmf
scenario: MTMF_DUAL_ATTENTION

# Data
use_real_stimuli: true
n_values: [1, 2, 3]
task_features: ["location", "identity", "category"]
sequence_length: 6
batch_size: 32
num_train: 20000
num_val: 200
num_test: 100
num_workers: 2
match_probability: 0.5

# Model
model_type: "attention"
hidden_size: 256
rnn_type: "gru"
num_layers: 1
dropout: 0.0
pretrained_backbone: true
freeze_backbone: true
classifier_layers: []

# Attention-specific parameters
attention_hidden_dim: 256
attention_dropout: 0.1
attention_mode: "dual"  # Gates depend on both task AND features

# Optim
epochs: 30
lr: 0.0001
weight_decay: 0.0001
milestones: [20, 25]
gamma: 0.1
grad_clip: 1.0
label_smoothing: 0.0
no_action_loss_weight: 0.1

# Validation
save_hidden: true
save_visualizations: true
mask_trivial_steps: true

# Data caching
cache_train_sequences: false  # false = fresh sequences each epoch (recommended)
cache_val_sequences: true  # true = consistent evaluation (recommended)
