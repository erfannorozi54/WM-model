# Attention-Enhanced Multi-Task Multi-Feature
experiment_name: wm_attention_mtmf
scenario: MTMF_ATTENTION

# Data
use_real_stimuli: true
n_values: [1, 2, 3]
task_features: ["location", "identity", "category"]
sequence_length: 6
batch_size: 8
num_train: 900
num_val: 200
num_test: 200
num_workers: 2

# Model
model_type: "attention_gru"  # attention_gru|attention_lstm|attention_rnn
hidden_size: 512
num_layers: 1
dropout: 0.0
pretrained_backbone: true
freeze_backbone: true
capture_exact_layer42_relu: true

# Attention-specific parameters
attention_hidden_dim: 512
attention_dropout: 0.1

# Optim
epochs: 15
lr: 3e-4
weight_decay: 1e-2
milestones: [10, 13]
gamma: 0.1
grad_clip: 1.0

# Validation
save_hidden: true
