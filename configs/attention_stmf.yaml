# Attention-Enhanced Single-Task Multi-Feature
experiment_name: wm_attention_stmf
scenario: STMF_ATTENTION

# Data
use_real_stimuli: true
n_values: [2]
task_features: ["location", "identity", "category"]
sequence_length: 6
batch_size: 8
num_train: 600
num_val: 150
num_test: 150
num_workers: 2

# Model
model_type: "attention_gru"  # attention_gru|attention_lstm|attention_rnn
hidden_size: 512
num_layers: 1
dropout: 0.0
pretrained_backbone: true
freeze_backbone: true
capture_exact_layer42_relu: true

# Attention-specific parameters
attention_hidden_dim: 512
attention_dropout: 0.1

# Optim
epochs: 12
lr: 3e-4
weight_decay: 1e-2
milestones: [8, 10]
gamma: 0.1
grad_clip: 1.0

# Validation
save_hidden: true
